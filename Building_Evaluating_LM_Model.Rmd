---
title: "Building_Evaluating_LM_Model"
author: "Michael Longstreth"
date: "June 23, 2019"
output: 
  html_document:
    toc: true
    number_sections: true
    toc_float: true
      
      
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objective
*  Use the dataset healthcare data frame (see below) to fit  
     +  an OLSR linear (lm) model
     +  a CART (rpart) model
     +  a Bootstapped model
     +  a Gradient Booting Machine ("GBM") model
*  to predict **costs**

*  Explore the dataset as needed to guide your analysis  
*  Prepare your dataset (e.g., to avoid overfitting)  
*  Fit your models using lm and rpart  
*  Tune your models to optimize performance    
*  Show and *explain* your results, use plots where appropriate  
*  Evaluate model performance  
*  State your conclusions 

```{r echo=TRUE, warning=FALSE, message=FALSE}
library(tidyverse)
healthcare <- read_csv('healthcare.csv')
```

# Examine raw data from data set.
```{r}
head(healthcare, n = 5)
tail(healthcare, n = 5)
dim(healthcare)
glimpse(healthcare)
```

# Convert categorical variable to factors and add binomial varible of same in advance of analysis.
```{r warning=FALSE, message=FALSE}
library(purrr)
healthcare[,c(2,5,6)] <- map(healthcare[,c(2,5,6)], as.factor)
healthcare$smoker_num <- ifelse(healthcare$smoker == "yes", 1, 0)
healthcare$gender_num <- ifelse(healthcare$gender == "male", 1, 0)
```

# View balance of categorical variables in dataset.
```{r warning=FALSE, message=FALSE}
library(gmodels)
CrossTable(healthcare$region)
CrossTable(healthcare$gender)
CrossTable(healthcare$smoker)
```

# Check for missing data; confirmed no data missing.
```{r}
sapply(healthcare, function(x) sum(is.na(x)))
```

# Do initial correlation check on variables, removing factors.
```{r}
healthcare_factors <- c(2, 5, 6)
cor(healthcare[,-healthcare_factors])
summary(healthcare)
```

# Perform initial analysis of variables in dataset using ggplot2.
```{r warning=FALSE, message=FALSE, comment=NA}
library(ggplot2)
#Histogram of Cost
ggplot(healthcare, aes(x = costs)) +
  geom_histogram(binwidth = 1000)

#Attempt to normalize distribution of cost variable
ggplot(healthcare, aes(x = log(costs))) +
  geom_histogram(binwidth = .1)

#Smoker fill on cost histogram, shows smokers highly concentrated in higher levels of cost distribution
ggplot(healthcare, aes(x = log(costs), fill = smoker)) +
  geom_histogram(binwidth = .1)

#Density Plots
ggplot(healthcare, aes(x = costs, fill = smoker)) +
  geom_density(alpha = .5) +
  scale_fill_manual(values=c("red","blue"))

ggplot(healthcare, aes(x = log(costs), fill = smoker)) +
  geom_density(alpha = .5) +
  scale_fill_manual(values=c("red","blue"))

#Plot of Age v. Costs showing distinct linear relationship (however there is jump in cost due to smoker = "Yes")
ggplot(healthcare, aes(x = age_yrs, y = costs, color = smoker)) +
  geom_point()
```

# Mean model for evaluation.
Retrieve the average cost across all variables and retrieve RMSE to use as a base line for future model results.
```{r}
mean_model <- mean(healthcare$costs) - healthcare[["costs"]]
rmse_mean_model <- sqrt(mean(mean_model^2))
model_performance <- list()
model_performance[1] <- rmse_mean_model
names(model_performance) <- c("Mean Model")
model_performance
```

# Train/Test set & model Formula.
Shuffle the data for prepartion of train/test sets, create train/test sets from shuffled data and create formula to be used for future models; from doing various analysis and checks on data, the variables and variable interactions in the code were selected.
```{r}
set.seed(42)
rows <- sample(nrow(healthcare))
healthcare <- healthcare[rows,]
assignment <- sample(1:3,
                     size = nrow(healthcare),
                     prob = c(.7, .15, .15),
                     replace = TRUE)
healthcare_train <- healthcare[assignment == 1,
                               -healthcare_factors]

healthcare_test <- healthcare[assignment == 2,
                              -healthcare_factors]

healthcare_valid <- healthcare[assignment == 3,
                               -healthcare_factors]
oslr_fm <- formula(costs ~
                     age_yrs +
                     body_mass +
                     no_children +
                     prev_balance +
                     smoker_num +
                     gender_num +
                     body_mass:smoker_num)
```

# OSLR Model
Create OSLR model and check RMSE performance.
```{r}
oslr_model <- lm(formula = oslr_fm,
                 data = healthcare_train)
summary(oslr_model)

oslr_predict <- predict(oslr_model,
                        newdata = healthcare_test)
oslr_error <- oslr_predict - healthcare_test[["costs"]]
rmse_oslr_model <- sqrt(mean(oslr_error^2))
model_performance[2] <- rmse_oslr_model
names(model_performance) <- c("Mean Model",
                              "OSLR Model")
model_performance
```

# "RPart" model.
Prepare "Rpart" regression tree model and check RMSE performance.
```{r warning=FALSE, message=FALSE}
rpart_fm <- formula(costs ~
                      age_yrs +
                      body_mass +
                      no_children +
                      prev_balance +
                      smoker_num +
                      gender_num)

library(rpart)
library(rpart.plot)
rpart_model <- rpart(formula = rpart_fm,
                     data = healthcare_train,
                     method = "anova")
summary(rpart_model)
rpart.plot(rpart_model)
rpart_predict <- predict(rpart_model,
                         newdata = healthcare_test)
rpart_error <- rpart_predict - healthcare_test[["costs"]]
rmse_rpart_model <- sqrt(mean(rpart_error^2))
model_performance[3] <- rmse_rpart_model
names(model_performance) <- c("Mean Model",
                              "OSLR Model",
                              "RPart Model")
model_performance
```

## CP Tuning.
Perform CP tuning on "RPart" model to try and improve performance.
```{r}
print(rpart_model$cptable)
opt_index <- which.min(rpart_model$cptable[, "xerror"])
cp_opt <- rpart_model$cptable[opt_index, "CP"]
rpart_model_cp <- prune(tree = rpart_model,
                        cp = cp_opt)
# rpart_model_cp
rpart_cp_predict <- predict(rpart_model_cp,
                            newdata = healthcare_test)
rpart_cp_error <- rpart_cp_predict - healthcare_test[["costs"]]
rmse_rpart_model_cp <- sqrt(mean(rpart_cp_error^2))
model_performance[4] <- rmse_rpart_model_cp
names(model_performance) <- c("Mean Model",
                              "OSLR Model",
                              "RPart Model",
                              "Rpart Model(CP)")
model_performance
```

## Hyper Grid tuning.
Perform Hyper Grid tuning on "RPart" model to try and improve performance.
```{r warning=FALSE, message=FALSE}
library(Metrics)
minsplit <- seq(1, 8, 1)
maxdepth <- seq(1, 6, 1)
hyper_grid <- expand.grid(minsplit = minsplit,
                          maxdepth = maxdepth)
head(hyper_grid)
nrow(hyper_grid)
num_rpart_models <- nrow(hyper_grid)
tune_rpart_models <- list()
for (i in 1:num_rpart_models) {
  minsplit <- hyper_grid$minsplit[i]
  maxdepth <- hyper_grid$maxdepth[i]
  
  tune_rpart_models[[i]] <- rpart(formula = rpart_fm,
                                  data = healthcare_train,
                                  method = "anova",
                                  minsplit = minsplit,
                                  maxdepth = maxdepth)
}
num_rpart_models <- length(tune_rpart_models)
rpart_rmse_values <- c()
for (i in 1:num_rpart_models) {
  model <- tune_rpart_models[[i]]
  
  pred <- predict(object = model,
                  newdata = healthcare_valid)
  
  rpart_rmse_values[i] <- rmse(actual = healthcare_valid$costs,
                               predicted = pred)
}
best_rpart_model <- tune_rpart_models[[which.min(rpart_rmse_values)]]
best_rpart_model$control
pred <- predict(object = best_rpart_model,
                newdata = healthcare_test)
rmse_rpart_model_tune <- rmse(actual = healthcare_test$costs,
                              predicted = pred)
model_performance[5] <- rmse_rpart_model_tune
names(model_performance) <- c("Mean Model",
                              "OSLR Model",
                              "RPart Model",
                              "Rpart Model(CP)",
                              "Rpart Model(Tune)")
model_performance
```

# Bootstrap model.
Perform Bootstrap model on dataset to try and improve model performance.
```{r warning=FALSE, message=FALSE}
library(ipred)
set.seed(42)
rpart_bag_model <- bagging(formula = rpart_fm,
                           data = healthcare_train,
                           coob = TRUE)
rpart_bag_model
rpart_bag_prediction <- predict(rpart_bag_model,
                                newdata = healthcare_test)
rmse_rpart_bootstrap <- rmse(actual = healthcare_test$costs,
                             predicted = rpart_bag_prediction)
model_performance[6] <- rmse_rpart_bootstrap
names(model_performance) <- c("Mean Model",
                              "OSLR Model",
                              "RPart Model",
                              "Rpart Model(CP)",
                              "Rpart Model(Tune)",
                              "Rpart Model(Bootstrap)")
model_performance
```

# GBM model.
Perform Gradient Boosting on dataset to try and improve model performan.
```{r warning=FALSE, message=FALSE}
library(gbm)
set.seed(42)
rpart_boost_model <- gbm(formula = rpart_fm,
                         distribution = "gaussian",
                         data = healthcare_train,
                         n.trees = 50000,
                         cv = 10)
gbm_opt <- gbm.perf(rpart_boost_model)
summary(rpart_boost_model)
rpart_boost_prediction <- predict(rpart_boost_model,
                                  newdata = healthcare_test,
                                  n.trees = gbm_opt)
rmse_rpart_boost_model <- rmse(actual = healthcare_test$costs,
                               predicted = rpart_boost_prediction)
model_performance[7] <- rmse_rpart_boost_model
names(model_performance) <- c("Mean Model",
                              "OSLR Model",
                              "RPart Model",
                              "Rpart Model(CP)",
                              "Rpart Model(Tune)",
                              "Rpart Model(Bootstrap)",
                              "Rpart Model(Boost)")
model_performance
```

# Conclusion.
Based on the RMSE the ideal model that was created is the Bootstrapped model, however,considering the RMSE, the initial OSLR model shows a significant RMSE when considering time and complexity of models.  With ime constraint and conveying analysis to Business Decision Makers, the OSLR model could be ideal.
Comments: Somewhat unknown as why tuning parameters did not improve "RPart" model and would like feedback on this.
